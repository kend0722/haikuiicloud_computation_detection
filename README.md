## conda activate HK9007
### 项目介绍:
2024/10/13开始重构了海葵的三维智慧园区算法检测平台。采用了海葵自研的分布式计算平台。
整个服务拆分为了三个部分，1：获取视频流，2：AI算法检测，3：其他的中间组件
#### 1. 获取视频流
##### 主要改进点：
1、对接三维智慧园区平台的数据交互，脱离了服务器。采用一个单独容器实现数据交互接口业务、每个摄像头的信息拼接好，存在Redis服务器中。（取代了此前的Manage父类，不会出现子类脏数据的情况，而且不需要添加心跳线程去维护传图线程）
2、采用fink分布式计算框架流来处理rtsp视频流，替换原来的SDK采集图像再利用socket长连接回传给算法检测客户端（减少内容占用，因为之前的内存开销很大）。fink算子直接对接Redis服务器去控制相关信息的更新。
3、取消了相似度容器，所有图像不在需要通过相似度容器处理后后再进行下一步的处理，而是直接在每个算子中完成这一块的业务，减少网络通信的压力
4、AI算法检测写成了一个单独的微服务，fink算子通过API请求，获得检测结果。(这里只需要传入hdfs地址(fink保存图像后生成的)和具体算法的参数信息(redis取数据,有效时间为70s。很好的监控摄像头的关闭，因为之前每次需要单独备份上一次的信息在内存中))
5、取消事件类，所有的检测结果不在统一的传到事件判定类。而是在fink中完成这一块的业务。(之前的事件类的网络压力是最大的，经常会丢数据，导致违规事件丢失)。
6、新增一个报警推送服务，也是在fink算子中完成。算子中把所有的违规事件的图像地址。请求到一个存图到浪潮服务器的API服务，且返回图像的挂载地址(4000个摄像头的视频流图像数据很夸张，hdfs只会保存最近十分钟的图,之前是将可能会违规的所有图像暂存在内存中)
7、hdfs存取延时高，每次违规后直接传一个hdfs地址的list。采用不会销毁的线程池去取图。(hdfs高可用模型下15张图时间开销在10s左右。优化后基本上在1.5S内)
8、利用K8s管理所有的浪潮服务器，监控容器的Pord。自动重启和新增所有的容器服务(保证fink流不会堵塞和，这里很重要的一个点是，分布式计算中的流。我们从一开始的架构就默认fink流是持续不断且不会发生堵塞的)
注意：fastapi压测后发现我们的一个容器。在并发数超过2000时，性能会下降很多。后面我们引入了Nginx代理服务实现负载均衡，且k8s可以根据Port的压力自动的帮我们调整资源。(这里不懂k8s是怎么实现的。是一个老运维帮我做的ai集群，致敬)
有个问题困扰我很长时间：为什么我们预采用的kafka。替换成了fastapi+Nginx的模式呢？
